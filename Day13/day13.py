# -*- coding: utf-8 -*-
"""Day13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z5DutJsjo7fXzY2TTeIYYp3CMAyFRma4

Tokenizers
"""

# ------------------------------------ Packages ----------------------------------
!pip install -q transformers

# ------------------------------------ Imports ----------------------------------
from google.colab import userdata
from huggingface_hub import login
from transformers import AutoTokenizer

# ------------------------------------ Configure Hugging Face Token ----------------------------------
# Retrieve stored API key from Colab's secure userdata store
hf_token = userdata.get('HF_TOKEN')

if hf_token:
    print(f"Hugging Face Token exists and begins {hf_token[:10]}")
else:
  print("Hugging Face Token not set")

# ------------------------------------ Connect to Hugging Face ----------------------------------
login(hf_token, add_to_git_credential=True)

# Request Access to HuggingFace Model:
# https://huggingface.co/black-forest-labs/FLUX.1-schnell

"""ðŸ¦™ Accessing LLaMA 3.1 from Meta

Meta's LLaMA 3.1 is an incredible open-weight large language model that you have to agree to their terms of service to use.  
  
ðŸ“„ Step 1: Accept Meta's Terms  
https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fmeta-llama%2FMeta-Llama-3.1-8B

    Go to the model page on Hugging Face:
    ðŸ‘‰ Meta-LLaMA-3.1-8B on Hugging Face

    At the top of the page, you'll find instructions to agree to Meta's terms.
    âœ… Use the same email address as your Hugging Face account for the smoothest experience.

ðŸ§  Step 2: Load the Model Using transformers

Meta's LLaMA models are compatible with the amazing ðŸ¤— transformers library â€” one of the most widely used tools for working with pre-trained machine learning models, especially in NLP.
âœ¨ Key Components

    AutoTokenizer

        A smart class from transformers that automatically selects the correct tokenizer based on the model.

        You donâ€™t need to know whether the model uses LlamaTokenizer, BertTokenizer, etc.

    .from_pretrained(...)

        This method downloads and loads the tokenizer or model weights from the Hugging Face Model Hub or a local path.

ðŸ§¬ Example Identifier

"meta-llama/Meta-Llama-3.1-8B"

This is the model ID used in Hugging Face to reference the 8B parameter version of LLaMA 3.1.
"""

# ------------------------------------ Connect to Meta-Llama-3.1-8B ----------------------------------
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)

"""Define the input text

text = "I am excited to show Tokenizers in action to my LLM engineers"

This is the sentence you want to tokenize â€” break into smaller units (called tokens) that the model can understand.

Encode the text

tokens = tokenizer.encode(text)

    The tokenizer.encode() method:

        Converts your input string into a list of token IDs.

        These are integers that represent the text as the model sees it.

        It automatically adds special tokens (like <s> or </s>) depending on the tokenizer configuration.

View the result

tokens

This outputs a list of integers like:

    [1, 72, 393, 2172, 281, 1262, 18196, 287, 389, 15548, 20571, 2]

    (Note: Actual output will vary based on which model/tokenizer is used.)

ðŸ“Œ Why Tokenization?

    LLMs (like LLaMA, GPT, BERT) don't process raw text â€” they work with numbers.

    Tokenization is the essential step that translates text into numerical input the model can work with.

ðŸ§  To convert the token IDs back to readable text:

decoded = tokenizer.decode(tokens)  
print(decoded)
"""

text = "I am excited to show Tokenizers in action to my LLM engineers"
tokens = tokenizer.encode(text)
tokens

# The count of the tokens that the text was encoded into.
len(tokens)

tokenizer.decode(tokens)

# Get the decoded text as a list of strings
tokenizer.batch_decode(tokens)

# tokenizer.vocab - Get the dictionary ID number of the token
tokenizer.get_added_vocab()

tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)

messages = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Tell a light-hearted joke for a room of Data Scientists"}
  ]

prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
print(prompt)

"""Example Use:"""

from transformers import AutoTokenizer, AutoModelForCausalLM

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True, device_map="auto")

# Define the chat messages
messages = [
    {"role": "system", "content": "You are a helpful assistant"},
    {"role": "user", "content": "Tell a light-hearted joke for a room of Data Scientists"}
]

# Create prompt
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# Tokenize prompt
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# Generate response
outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.9)

# Decode and print
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(response)

"""Trying 3 Different Models:

Phi3 from Microsoft Qwen2 from Alibaba Cloud Starcoder2 from BigCode (ServiceNow + HuggingFace + NVidia)
"""

PHI3_MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
QWEN2_MODEL_NAME = "Qwen/Qwen2-7B-Instruct"
STARCODER2_MODEL_NAME = "bigcode/starcoder2-3b"

phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)

text = "I am excited to show Tokenizers in action to my LLM engineers"
print(tokenizer.encode(text))
print()
tokens = phi3_tokenizer.encode(text)
print(phi3_tokenizer.batch_decode(tokens))

print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
print()
print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))

qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)

text = "I am excited to show Tokenizers in action to my LLM engineers"
print(tokenizer.encode(text))
print()
print(phi3_tokenizer.encode(text))
print()
print(qwen2_tokenizer.encode(text))

print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
print()
print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))
print()
print(qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))

starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)
code = """
def hello_world(person):
  print("Hello", person)
"""
tokens = starcoder2_tokenizer.encode(code)
for token in tokens:
  print(f"{token}={starcoder2_tokenizer.decode(token)}")