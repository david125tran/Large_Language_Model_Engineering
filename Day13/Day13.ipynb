{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers"
      ],
      "metadata": {
        "id": "DCkXZ9BjuZMt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RrmA2yiuVNz"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------ Packages ----------------------------------!\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Imports ----------------------------------\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "S0PekmI8ug2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Configure Hugging Face Token ----------------------------------\n",
        "# Retrieve stored API key from Colab's secure userdata store\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "if hf_token:\n",
        "    print(f\"Hugging Face Token exists and begins {hf_token[:10]}\")\n",
        "else:\n",
        "  print(\"Hugging Face Token not set\")\n"
      ],
      "metadata": {
        "id": "4lcuebpgv9Hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Connect to Hugging Face ----------------------------------\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "# Request Access to HuggingFace Model:\n",
        "# https://huggingface.co/black-forest-labs/FLUX.1-schnell"
      ],
      "metadata": {
        "id": "hsLz838Tv9ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ¦™ Accessing LLaMA 3.1 from Meta\n",
        "\n",
        "Meta's LLaMA 3.1 is an incredible open-weight large language model that you have to agree to their terms of service to use.  \n",
        "  \n",
        "ðŸ“„ Step 1: Accept Meta's Terms  \n",
        "https://www.google.com/url?q=https%3A%2F%2Fhuggingface.co%2Fmeta-llama%2FMeta-Llama-3.1-8B\n",
        "\n",
        "    Go to the model page on Hugging Face:\n",
        "    ðŸ‘‰ Meta-LLaMA-3.1-8B on Hugging Face\n",
        "\n",
        "    At the top of the page, you'll find instructions to agree to Meta's terms.\n",
        "    âœ… Use the same email address as your Hugging Face account for the smoothest experience.\n",
        "\n",
        "ðŸ§  Step 2: Load the Model Using transformers\n",
        "\n",
        "Meta's LLaMA models are compatible with the amazing ðŸ¤— transformers library â€” one of the most widely used tools for working with pre-trained machine learning models, especially in NLP.\n",
        "âœ¨ Key Components\n",
        "\n",
        "    AutoTokenizer\n",
        "\n",
        "        A smart class from transformers that automatically selects the correct tokenizer based on the model.\n",
        "\n",
        "        You donâ€™t need to know whether the model uses LlamaTokenizer, BertTokenizer, etc.\n",
        "\n",
        "    .from_pretrained(...)\n",
        "\n",
        "        This method downloads and loads the tokenizer or model weights from the Hugging Face Model Hub or a local path.\n",
        "\n",
        "ðŸ§¬ Example Identifier\n",
        "\n",
        "\"meta-llama/Meta-Llama-3.1-8B\"\n",
        "\n",
        "This is the model ID used in Hugging Face to reference the 8B parameter version of LLaMA 3.1."
      ],
      "metadata": {
        "id": "Mz7eG-lNnJQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Connect to Meta-Llama-3.1-8B ----------------------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "grDzNfCPv9W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the input text\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "\n",
        "This is the sentence you want to tokenize â€” break into smaller units (called tokens) that the model can understand.\n",
        "\n",
        "Encode the text\n",
        "\n",
        "tokens = tokenizer.encode(text)\n",
        "\n",
        "    The tokenizer.encode() method:\n",
        "\n",
        "        Converts your input string into a list of token IDs.\n",
        "\n",
        "        These are integers that represent the text as the model sees it.\n",
        "\n",
        "        It automatically adds special tokens (like <s> or </s>) depending on the tokenizer configuration.\n",
        "\n",
        "View the result\n",
        "\n",
        "tokens\n",
        "\n",
        "This outputs a list of integers like:\n",
        "\n",
        "    [1, 72, 393, 2172, 281, 1262, 18196, 287, 389, 15548, 20571, 2]\n",
        "\n",
        "    (Note: Actual output will vary based on which model/tokenizer is used.)\n",
        "\n",
        "ðŸ“Œ Why Tokenization?\n",
        "\n",
        "    LLMs (like LLaMA, GPT, BERT) don't process raw text â€” they work with numbers.\n",
        "\n",
        "    Tokenization is the essential step that translates text into numerical input the model can work with.\n",
        "\n",
        "ðŸ§  To convert the token IDs back to readable text:\n",
        "\n",
        "decoded = tokenizer.decode(tokens)  \n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "qwamCH02nNke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "tokens = tokenizer.encode(text)\n",
        "tokens"
      ],
      "metadata": {
        "id": "FLlXhTRSnPIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The count of the tokens that the text was encoded into.\n",
        "len(tokens)"
      ],
      "metadata": {
        "id": "Q22xRuyxnQKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokens)"
      ],
      "metadata": {
        "id": "T2F3oj3tnSpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the decoded text as a list of strings\n",
        "tokenizer.batch_decode(tokens)"
      ],
      "metadata": {
        "id": "rixmQ9HynT83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer.vocab - Get the dictionary ID number of the token\n",
        "tokenizer.get_added_vocab()"
      ],
      "metadata": {
        "id": "FpMcz8S2nVAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)"
      ],
      "metadata": {
        "id": "Usl5gJtTnXVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "print(prompt)\n",
        "\n"
      ],
      "metadata": {
        "id": "dBGRUb5KnYi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Use:"
      ],
      "metadata": {
        "id": "MUsjNKx5na4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True, device_map=\"auto\")\n",
        "\n",
        "# Define the chat messages\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "]\n",
        "\n",
        "# Create prompt\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Tokenize prompt\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate response\n",
        "outputs = model.generate(**inputs, max_new_tokens=256, do_sample=True, temperature=0.7, top_p=0.9)\n",
        "\n",
        "# Decode and print\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "JZ_FStohnYcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trying 3 Different Models:\n",
        "\n",
        "Phi3 from Microsoft Qwen2 from Alibaba Cloud Starcoder2 from BigCode (ServiceNow + HuggingFace + NVidia)"
      ],
      "metadata": {
        "id": "SN1mKYIund9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
        "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
      ],
      "metadata": {
        "id": "jn4tVc00ne0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "tokens = phi3_tokenizer.encode(text)\n",
        "print(phi3_tokenizer.batch_decode(tokens))\n"
      ],
      "metadata": {
        "id": "dVuV2X6tngLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "JO49CpkFnhq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qwen2_tokenizer = AutoTokenizer.from_pretrained(QWEN2_MODEL_NAME)\n",
        "\n",
        "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
        "print(tokenizer.encode(text))\n",
        "print()\n",
        "print(phi3_tokenizer.encode(text))\n",
        "print()\n",
        "print(qwen2_tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "bJI39JQ4nkUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
        "print()\n",
        "print(qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))"
      ],
      "metadata": {
        "id": "n9mos2RbniyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n",
        "code = \"\"\"\n",
        "def hello_world(person):\n",
        "  print(\"Hello\", person)\n",
        "\"\"\"\n",
        "tokens = starcoder2_tokenizer.encode(code)\n",
        "for token in tokens:\n",
        "  print(f\"{token}={starcoder2_tokenizer.decode(token)}\")"
      ],
      "metadata": {
        "id": "6JkoHQ_EnpgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xyx8cEF_npPe"
      }
    }
  ]
}