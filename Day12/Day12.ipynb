{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üåü HuggingFace Transformers Library\n",
        "\n",
        "üîó [https://huggingface.co](https://huggingface.co)\n",
        "\n",
        "The HuggingFace Transformers library provides APIs at two different levels to make working with state-of-the-art models easier.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ High-Level API: `pipeline`\n",
        "\n",
        "The high-level API for using open-source models for typical inference tasks is called **`pipeline`**.\n",
        "\n",
        "You can create and use a pipeline with just a few lines of code:\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Define the task you want to perform\n",
        "my_pipeline = pipeline(\"the_task_I_want_to_do\")\n",
        "\n",
        "# Run the pipeline on your input\n",
        "result = my_pipeline(my_input)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZjDFxaVvKNF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üõë Common Runtime Error in Google Colab\n",
        "\n",
        "While running a notebook, you might encounter an error like this:\n",
        "\n",
        "Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "    ‚ö†Ô∏è Don‚Äôt be misled!\n",
        "    This message might suggest that a package version issue is the cause, but that‚Äôs usually not true.\n",
        "\n",
        "üí° What‚Äôs Actually Happening:\n",
        "\n",
        "Google Colab may silently switch your runtime environment‚Äîespecially during high demand. This results in losing access to the GPU, which triggers that misleading error.  \n",
        "  \n",
        "‚úÖ How to Fix It\n",
        "\n",
        "Follow these steps to resolve the issue:\n",
        "\n",
        "    Kernel menu ‚Üí Disconnect and delete runtime\n",
        "\n",
        "    Reload the Colab notebook\n",
        "\n",
        "    Edit menu ‚Üí Clear all outputs\n",
        "\n",
        "    Reconnect to a GPU:\n",
        "\n",
        "        Click the \"Connect\" button in the top-right corner\n",
        "\n",
        "        Select \"Change runtime type\" if needed, and ensure GPU is selected\n",
        "\n",
        "    Confirm GPU availability:\n",
        "\n",
        "        Click \"View resources\" in the top-right menu\n",
        "\n",
        "        Check that a GPU (like a T4) is listed\n",
        "\n",
        "    Rerun all cells from the top, starting with the pip install commands\n",
        "\n",
        "üß† Pro Tip: Always run the install and setup cells at the beginning after resetting the runtime to ensure everything loads correctly.\n",
        "  \n",
        "  "
      ],
      "metadata": {
        "id": "oHk1o2znLaM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî• All Available Pipelines\n",
        "\n",
        "Here‚Äôs a list of all the pipelines available from **Transformers** and **Diffusers**.\n",
        "\n",
        "---\n",
        "\n",
        "### üåü Transformers Pipelines\n",
        "You can explore the available pipelines under the *Tasks* section on the HuggingFace Transformers documentation page.\n",
        "\n",
        "- Visit: [Transformers Pipelines Documentation](https://huggingface.co/docs/transformers/main_classes/pipelines)\n",
        "- Scroll down a bit, and expand the **parameters** section to see the list of **Tasks**.\n",
        "\n",
        "### üåü Diffusion Pipelines\n",
        "In case you‚Äôre interested in **Diffusion models** (e.g., image generation tasks), here‚Äôs the list for those.\n",
        "\n",
        "- Visit: [Diffusers Pipelines Documentation](https://huggingface.co/docs/diffusers/en/api/pipelines/overview)\n",
        "\n",
        "---\n",
        "\n",
        "If you come across any cool examples of other pipelines, **please share them** with me! It‚Äôs amazing how HuggingFace makes this **advanced AI functionality** available for inference with such a **simple API**. ü§ñ‚ú®\n"
      ],
      "metadata": {
        "id": "TS3GC_d5L8Xd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYRdOLl4rUty"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------ Packages ----------------------------------!\n",
        "!pip install -q -U transformers datasets diffusers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Imports ----------------------------------\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline\n",
        "from diffusers import DiffusionPipeline\n",
        "from datasets import load_dataset\n",
        "import soundfile as sf\n",
        "from IPython.display import Audio"
      ],
      "metadata": {
        "id": "l2dMNeXUrelU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Configure API Keys / Tokens ----------------------------------\n",
        "# Retrieve stored API keys from Colab's secure userdata store\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "anthropic_api_key = userdata.get('ANTHROPIC_API_KEY')\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "print(\"API Keys:\")\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:10]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "\n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:10]}\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set\")\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:10]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set\")\n",
        "if hf_token:\n",
        "    print(f\"Hugging Face Token exists and begins {hf_token[:10]}\")\n",
        "else:\n",
        "  print(\"Hugging Face Token not set\")\n"
      ],
      "metadata": {
        "id": "EhEz3ay5rjVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------------------------ Connect to Hugging Face ----------------------------------\n",
        "login(hf_token, add_to_git_credential=True)\n",
        "\n",
        "# Request Access to HuggingFace Model:\n",
        "# https://huggingface.co/black-forest-labs/FLUX.1-schnell"
      ],
      "metadata": {
        "id": "c8riT194rzw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Sentiment Analysis ----------------------------------\n",
        "classifier = pipeline(\"sentiment-analysis\", device=\"cuda\")\n",
        "result = classifier(\"I'm super excited to go eat at Larry's Steakhouse!\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "3e7pAFBVr8p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Name Entity Recognition (ner) ----------------------------------\n",
        "ner = pipeline(\"ner\", grouped_entities=True, device=\"cuda\")\n",
        "result = ner(\"Barack Obama was the 44th president of the United States.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "9pT_Alyor9Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Question Answering with Context ----------------------------------\n",
        "question_answerer = pipeline(\"question-answering\", device=\"cuda\")\n",
        "result = question_answerer(question=\"Who found dinosaur bones in Raleigh, NC in 2023?\", context=\"David Tran found dinosaur fossils in Raleigh, NC in 2023.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "TdrUxhIfr9dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Text Summarization ----------------------------------\n",
        "summarizer = pipeline(\"summarization\", device=\"cuda\")\n",
        "text = \"\"\"Common art themes reflect universal human experiences and concerns, transcending time and culture.\n",
        "These often include the human condition, exploring aspects of identity, emotion, life, and death. Nature is\n",
        "another pervasive theme, depicted in landscapes, seascapes, or as a symbol of growth, decay, or the sublime.\n",
        "Religion and spirituality have historically been dominant, portraying deities, sacred narratives, and the\n",
        "search for meaning. Social and political commentary frequently emerges, addressing conflict, injustice,\n",
        "freedom, or the power dynamics within society. Finally, storytelling and history remain vital, with art\n",
        "serving as a record of significant events, myths, and personal narratives.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "S4R6Vl5EJDAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Translation ----------------------------------\n",
        "translator = pipeline(\"translation_en_to_fr\", device=\"cuda\")\n",
        "result = translator(\"The finest Sushi in North Carolina is actually in a restaurant located inside of a gas station!\")\n",
        "print(result[0]['translation_text'])"
      ],
      "metadata": {
        "id": "Pss0V5K0JCUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Translation w/a Specified Model ----------------------------------\n",
        "translator = pipeline(\"translation_en_to_es\", model=\"Helsinki-NLP/opus-mt-en-es\", device=\"cuda\")\n",
        "result = translator(\"The finest Sushi in North Carolina is actually in a restaurant located inside of a gas station!\")\n",
        "print(result[0]['translation_text'])"
      ],
      "metadata": {
        "id": "oC6BxETQJGCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Classification ----------------------------------\n",
        "classifier = pipeline(\"zero-shot-classification\", device=\"cuda\")\n",
        "result = classifier(\"UFO reports are currently circulating in Raleigh, NC.\", candidate_labels=[\"technology\", \"sports\", \"politics\"])\n",
        "print(result)"
      ],
      "metadata": {
        "id": "wMQXbAo-JcBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Text Generation ----------------------------------\n",
        "generator = pipeline(\"text-generation\", device=\"cuda\")\n",
        "result = generator(\"If there's one thing I you should about using HuggingFace pipelines, it's\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "c9QiG1wJJg4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Image Generation ----------------------------------\n",
        "image_gen = DiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-diffusion-2\",\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "text = \"A dog painting a picture of a fish, in the surreal style of Salvador Dali\"\n",
        "image = image_gen(prompt=text).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "qOWculHdJgvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Audio Generation ----------------------------------\n",
        "synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
        "\n",
        "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
        "speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
        "\n",
        "speech = synthesiser(\"Hello World, from the Hugging Face pipeline!\", forward_params={\"speaker_embeddings\": speaker_embedding})\n",
        "\n",
        "sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])\n",
        "Audio(\"speech.wav\")"
      ],
      "metadata": {
        "id": "qzWdQZk1Jgfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCAss1kaJi5a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}