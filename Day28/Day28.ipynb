{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Mastering Parameter-Efficient Fine-Tuning (PEFT)\n",
        "\n",
        "Focusing on:\n",
        "\n",
        "## ðŸ”§ LoRA (Low-Rank Adaptation)\n",
        "\n",
        "### âœ”ï¸ Problem\n",
        "Many LLMs have **billions of parameters**, making them expensive to fine-tune. These models are made up of many layers (e.g., 32 decoder layers in transformers), each including:\n",
        "\n",
        "- ðŸ¤ **Self-Attention**: Learns relationships between words  \n",
        "- ðŸ§  **MLP (Multi-Layer Perceptron)**: Learns complex patterns  \n",
        "- ðŸŒŠ **SiLU Activation**: A smooth activation function  \n",
        "- ðŸ§½ **LayerNorm**: Keeps values stable during training  \n",
        "\n",
        "### âœ”ï¸ Solution\n",
        "LoRA allows fine-tuning by **adding small trainable matrices** to the frozen base model weights.\n",
        "\n",
        "- â„ï¸ Freeze original model weights  \n",
        "- ðŸ§© Only train small adapter matrices  \n",
        "- ðŸ“‰ Drastically reduces trainable parameters  \n",
        "- ðŸ” LoRA adapters are pluggable into any transformer model  \n",
        "\n",
        "### âœ”ï¸ How it works\n",
        "Weight updates are **decomposed into low-rank matrices** (A & B).\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ QLoRA\n",
        "\n",
        "### âœ”ï¸ Problem\n",
        "Fine-tuning large models = **high memory & compute cost**.\n",
        "\n",
        "### âœ”ï¸ Solution  \n",
        "QLoRA combines two major tricks:\n",
        "\n",
        "1. ðŸ§® **Quantization** â€“ Reduces memory (e.g., 16-bit â†’ 4-bit = 75% less memory)  \n",
        "2. ðŸ§© **LoRA** â€“ Enables large model tuning on consumer GPUs\n",
        "\n",
        "### âœ”ï¸ How it works\n",
        "\n",
        "- ðŸ§Š Load quantized base model (lower bit precision)  \n",
        "- â„ï¸ Freeze base model weights  \n",
        "- ðŸ’¡ Apply **full-precision LoRA adapters** for fine-tuning  \n",
        "- ðŸ§  Adapters learn effectively while base stays efficient\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¬ Hyperparameters\n",
        "\n",
        "### âœ”ï¸ Problem  \n",
        "Optimal fine-tuning requires **careful hyperparameter tuning**.\n",
        "\n",
        "### âœ”ï¸ Solution  \n",
        "Hyperparameters are tunable **training settings** â€” not part of the model architecture.\n",
        "\n",
        "### âœ”ï¸ How it works â€“ Key QLoRA Hyperparameters:\n",
        "\n",
        "- `r` (rank): ðŸ§® Size of A & B matrices (e.g., 5, 8, 16)  \n",
        "- `lora_alpha`: ðŸ”§ Scaling factor for updates (e.g., 16, 32, 64)  \n",
        "- `lora_dropout`: ðŸŽ² Drop adapter weights randomly (e.g., 0.05, 0.1)  \n",
        "- `target_modules`: ðŸŽ¯ Where to apply LoRA (e.g., `q_proj`, `v_proj`)  \n",
        "- `bias`: âš–ï¸ Bias handling (e.g., `\"none\"`, `\"lora_only\"`, `\"all\"`)\n",
        "\n",
        "### âœ”ï¸ How hyperparameters affect performance:\n",
        "\n",
        "- âš¡ Want faster training? â†’ Increase `learning_rate` (carefully)  \n",
        "- ðŸ›¡ï¸ Overfitting? â†’ Add `lora_dropout`, reduce `num_train_epochs`  \n",
        "- ðŸ“‰ Underfitting? â†’ Increase `r`, `lora_alpha`, or `epochs`  \n",
        "- ðŸ’¾ Low on memory? â†’ Reduce `r`, `batch_size`, or increase `gradient_accumulation_steps`\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Benefits\n",
        "\n",
        "âœ”ï¸ Drastically reduces memory & compute needs  \n",
        "âœ”ï¸ Enables scalable, low-cost fine-tuning of LLMs  \n",
        "âœ”ï¸ Great for task-specific adaptation (e.g., domain-specific QA, LCMS lab data)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸš€ Use PEFT to:\n",
        "\n",
        "- ðŸ§ª Customize LLMs for **niche domains**  \n",
        "- ðŸ’» Deploy powerful models in **resource-constrained environments**  \n",
        "- ðŸ” Fine-tune **multiple adapters** for different tasks or use cases  \n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ—‚ï¸ Summary Table\n",
        "\n",
        "| **Concept**         | **What It Does**                                 | **Relationship**                          |\n",
        "|---------------------|--------------------------------------------------|-------------------------------------------|\n",
        "| ðŸ§  **PEFT**         | Efficient fine-tuning strategy                   | LoRA and QLoRA are PEFT methods           |\n",
        "| ðŸ§© **LoRA**         | Adds small adapter matrices (A & B)              | A specific PEFT method                    |\n",
        "| ðŸ§® **QLoRA**        | LoRA + 4-bit quantized base model                | A memory-efficient extension of LoRA      |\n",
        "| âš™ï¸ **Hyperparameters** | Control training behavior                         | Used in both LoRA & QLoRA setups           |\n"
      ],
      "metadata": {
        "id": "CfrkDB69DSAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Packages ----------------------------------\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
        "!pip install -q datasets requests peft"
      ],
      "metadata": {
        "id": "rrTIpSnNDeca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDyR63OTNUJ6"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
        "!pip install -q datasets requests peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Imports ----------------------------------\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "e-GGLqdwDn5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
        "\n",
        "# Hyperparameters for QLoRA Fine-Tuning\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
      ],
      "metadata": {
        "id": "N-qersEHDv6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to HuggingFace\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "v9Hi-OFgD7XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Load (No Quantization) Base Model (initial test, full precision) ----------------------------------\n",
        "# QLoRA loads base model in 4-bit to save memory\n",
        "print(\"\\nðŸ“¦ Loading 4-bit Quantized Base Model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")\n",
        "print(f\"ðŸ§  4-bit Quantized Base Model Memory Footprint: {base_model.get_memory_footprint() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "S78urJVkD9n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (installs and imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ],
      "metadata": {
        "id": "hkYz_MCoMW_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Load 8-bit Quantized Model (first optimization) ----------------------------------\n",
        "print(\"\\nðŸ“¦ Loading 8-bit Quantized Base Model...\")\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "print(f\"ðŸ§  8-bit Quantized Base Model Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ],
      "metadata": {
        "id": "iNWcL836EMs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Load 4-bit Quantized Model (QLoRA) ----------------------------------\n",
        "print(\"\\nðŸ“¦ Loading 4-bit Quantized Model (QLoRA) Model...\")\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"  # NormalFloat4 format: more accurate for LLMs\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, quantization_config=quant_config, device_map=\"auto\")\n",
        "print(f\"ðŸ§  4-bit Quantized Model (QLoRA) Model Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ],
      "metadata": {
        "id": "k74u5AbnEOSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ Load Fine-Tuned LoRA Adapters ----------------------------------\n",
        "print(\"\\nðŸ“¦ Loading Fine-Tuned LoRA Adapter...\")\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)\n",
        "print(f\"ðŸ§  Fine-Tuned Model Memory Footprint: {fine_tuned_model.get_memory_footprint() / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "dFTM1_qfEQaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------ LoRA Parameter Analysis ----------------------------------\n",
        "print()\n",
        "# Estimate LoRA adapter parameter counts for each attention projection\n",
        "# LoRA introduces 2 matrices (A and B) per target module\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32\n",
        "\n",
        "# Total parameters for one transformer block (layer)\n",
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "\n",
        "# Total layers in LLaMA 8B = 32 transformer blocks\n",
        "params = lora_layer * 32\n",
        "\n",
        "# Estimate total adapter size in MB (4 bytes per FP32 parameter)\n",
        "size = (params * 4) / 1_000_000\n",
        "print(f\"\\nðŸ“Š LoRA Adapter Params: {params:,}\")\n",
        "print(f\"ðŸ’¾ Approx. LoRA Adapter Size: {size:.1f} MB\")\n"
      ],
      "metadata": {
        "id": "fXlLyw45EQMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gAq0AxKFnD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}